{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712fb4ec-1f43-489a-b4c0-90c0066364b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import chardet\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b33c44b-54e5-49ab-9f16-b21b338f861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/hate_speech.tsv\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read(100000))  # Read first 100KB\n",
    "    print(result[\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd4ccb0-23d0-4bcc-924c-bc5029b37fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codemixed</th>\n",
       "      <th>HateOrNot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>ye attankwadi Indian agent hai jo terrorism ph...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>bola na terrorism ko support karna band karoge...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>lagta hai aap ne movie dekhi hai which is writ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>tum log terrorism ko support karna band kardo ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>mujhe pehele se hi pata tha so Sallu fans ke b...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Codemixed HateOrNot\n",
       "4574  ye attankwadi Indian agent hai jo terrorism ph...        no\n",
       "4575  bola na terrorism ko support karna band karoge...        no\n",
       "4576  lagta hai aap ne movie dekhi hai which is writ...        no\n",
       "4577  tum log terrorism ko support karna band kardo ...        no\n",
       "4578  mujhe pehele se hi pata tha so Sallu fans ke b...       yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\"Codemixed\", \"HateOrNot\"]  # Replace with actual column names\n",
    "hs_df = pd.read_csv(\"dataset/hate_speech.tsv\", \n",
    "                    encoding=result[\"encoding\"], \n",
    "                    sep=\"\\t\", header=None, \n",
    "                    names=column_names)\n",
    "\n",
    "hs_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0e10fb-397e-4c6a-9fad-cf08ebdb76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HateOrNot\n",
      "no     2914\n",
      "yes    1661\n",
      "n         2\n",
      "on        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(hs_df[\"HateOrNot\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ff5981-bb6e-4126-bbc5-fe1bb7cd528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_codemixed_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)         # remove links\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)                   # remove @ and the word after it\n",
    "    text = re.sub(r\"#\", \"\", text)                      # remove just the '#' symbol\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()           # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "hs_df[\"Codemixed\"] = hs_df[\"Codemixed\"].astype(str).apply(clean_codemixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31007c55-a4be-4bef-b35d-171aaa4a62ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4573    pehle confirm karo ke Mohammad ne sach mein 8 ...\n",
       "4574    ye attankwadi Indian agent hai jo terrorism ph...\n",
       "4575    bola na terrorism ko support karna band karoge...\n",
       "4576    lagta hai aap ne movie dekhi hai which is writ...\n",
       "4577    tum log terrorism ko support karna band kardo ...\n",
       "Name: Codemixed, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df[\"Codemixed\"][-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e80d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_df['Codemixed'].to_csv('code_mixed.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a5a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=code_mixed.txt --model_prefix=m --vocab_size=5000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: code_mixed.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: code_mixed.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4579 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=468250\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=74\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4579 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=221590\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 24265 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4579\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 20113\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 20113 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12103 obj=11.1709 num_tokens=42534 num_tokens/piece=3.51434\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10641 obj=10.0563 num_tokens=42876 num_tokens/piece=4.02932\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7976 obj=10.0613 num_tokens=44951 num_tokens/piece=5.63578\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7965 obj=10.0061 num_tokens=44947 num_tokens/piece=5.64306\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5973 obj=10.1513 num_tokens=48118 num_tokens/piece=8.05592\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5973 obj=10.0932 num_tokens=48147 num_tokens/piece=8.06077\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5500 obj=10.1395 num_tokens=49025 num_tokens/piece=8.91364\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5500 obj=10.1228 num_tokens=49027 num_tokens/piece=8.914\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=code_mixed.txt --model_prefix=m --vocab_size=5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e06d5a0-98be-424a-bc7b-304dd33c587b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34019769-1a45-4653-94a6-07337144bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3205\n",
      "Test samples: 1374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "hs_df[\"HateOrNot\"] = hs_df[\"HateOrNot\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "train_df, test_df = train_test_split(hs_df, test_size=0.3, random_state=42, stratify=hs_df[\"HateOrNot\"])\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d21b2d-b58e-40c6-994e-36f839e3fc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments processed: 4579\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "vocab_size = 8000          # SentencePiece vocab size\n",
    "embedding_dim = 768        \n",
    "max_seq_len = 512          # Max number of tokens per comment\n",
    "\n",
    "# 1. Create embedding layers \n",
    "token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "pos_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "# Define one transformer encoder layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embedding_dim,         # 768 ‚Äî dimension of embeddings\n",
    "    nhead=8,                        # 8 attention heads\n",
    "    dim_feedforward=2048,          # size of feedforward layer\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    batch_first=True               # set to True to use [batch_size, seq_len, dim]\n",
    ")\n",
    "\n",
    "# Stack 12 such layers\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)\n",
    "\n",
    "# 2. Container for final embeddings\n",
    "contextual_embeddings = {}\n",
    "\n",
    "for i, comment in enumerate(hs_df['Codemixed']):\n",
    "    ids = sp.encode_as_ids(comment)\n",
    "    if not ids:\n",
    "        continue\n",
    "\n",
    "    token_ids = torch.tensor(ids)\n",
    "    positions = torch.arange(len(token_ids))\n",
    "\n",
    "    # Get initial embeddings\n",
    "    token_embed_output = token_embedding(token_ids)\n",
    "    position_embed_output = pos_embedding(positions)\n",
    "    final_embedding = token_embed_output + position_embed_output  # [seq_len, 768]\n",
    "\n",
    "    # Add batch dimension: [1, seq_len, 768]\n",
    "    input_tensor = final_embedding.unsqueeze(0)\n",
    "\n",
    "    # Pass through 12-layer transformer\n",
    "    with torch.no_grad():  # unless you're training\n",
    "        transformer_output = transformer_encoder(input_tensor)  # [1, seq_len, 768]\n",
    "\n",
    "    # Option 1: Use mean pooling to get a single vector for the whole comment\n",
    "    contextual_embeddings[i] = transformer_output.squeeze(0)\n",
    "\n",
    "print(\"Total comments processed:\", len(contextual_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0008c36c-020d-4110-b81b-ee3ef658949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_size=128):\n",
    "        super(HANEncoder, self).__init__()\n",
    "        self.bigru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.context_vector = nn.Parameter(torch.rand(hidden_size * 2))\n",
    "\n",
    "    def forward(self, embeddings):  # [seq_len, input_dim]\n",
    "        x = embeddings.unsqueeze(0)  # [1, seq_len, input_dim]\n",
    "        gru_out, _ = self.bigru(x)   # [1, seq_len, 2*hidden]\n",
    "\n",
    "        u = torch.tanh(self.attention_fc(gru_out))         # [1, seq_len, 2*hidden]\n",
    "        alpha = torch.matmul(u, self.context_vector)       # [1, seq_len]\n",
    "        alpha = F.softmax(alpha, dim=-1)                   # [1, seq_len]\n",
    "\n",
    "        # Weighted sum\n",
    "        sentence_vector = torch.sum(gru_out * alpha.unsqueeze(-1), dim=1)  # [1, 2*hidden]\n",
    "\n",
    "        return sentence_vector.squeeze(0), alpha.squeeze(0)  # return both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886b0f0d-984f-4685-8c32-9882c4081add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4579/4579 [00:04<00:00, 1018.79it/s]\n"
     ]
    }
   ],
   "source": [
    "han_model = HANEncoder(input_dim=768, hidden_size=128)\n",
    "han_model.eval()\n",
    "\n",
    "final_vectors = {}\n",
    "attention_scores = {}\n",
    "\n",
    "for idx, embedding in tqdm(contextual_embeddings.items()):\n",
    "    if embedding.shape[0] < 2:\n",
    "        continue  # skip very short ones\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sentence_vector, attn_weights = han_model(embedding)\n",
    "\n",
    "        final_vectors[idx] = sentence_vector  # [256]\n",
    "        attention_scores[idx] = attn_weights  # [seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983e6dfe-7f75-4761-9f92-7e8fb80e5aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped idx 3689: label was 'n' (not yes/no)\n",
      "Skipped idx 3763: label was 'on' (not yes/no)\n",
      "Skipped idx 3874: label was 'n' (not yes/no)\n",
      "Skipped idx 4407: label was 'on' (not yes/no)\n",
      "\n",
      " Final usable vectors: 4575\n",
      " Skipped: 4\n"
     ]
    }
   ],
   "source": [
    "label_map = {\"yes\": 1, \"no\": 0}\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "skipped = 0\n",
    "\n",
    "for idx, vec in final_vectors.items():\n",
    "    raw_label = hs_df.loc[idx, 'HateOrNot']\n",
    "    label = label_map.get(str(raw_label).strip().lower())\n",
    "\n",
    "    if label is None:\n",
    "        print(f\"Skipped idx {idx}: label was '{raw_label}' (not yes/no)\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    if not isinstance(vec, torch.Tensor):\n",
    "        print(f\"Skipped idx {idx}: vector is not a tensor\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    if vec.shape != torch.Size([256]):\n",
    "        print(f\"Skipped idx {idx}: vector has shape {vec.shape} (expected [256])\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    X.append(vec)\n",
    "    y.append(label)\n",
    "\n",
    "print(f\"\\n Final usable vectors: {len(X)}\")\n",
    "print(f\" Skipped: {skipped}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03582fe-50ce-447e-86dc-04ebff2995a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_vectors type: <class 'dict'>\n",
      "final_vectors length: 4579\n",
      "Key: 0, Shape: torch.Size([256]), Type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(\"final_vectors type:\", type(final_vectors))\n",
    "print(\"final_vectors length:\", len(final_vectors))\n",
    "\n",
    "# Print a sample entry if exists\n",
    "for k, v in final_vectors.items():\n",
    "    print(f\"Key: {k}, Shape: {getattr(v, 'shape', 'Not a tensor')}, Type: {type(v)}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9bfade5-4aa3-4830-9e40-6faf45c5e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.8882\n",
      "Epoch 2 - Loss: 0.8869\n",
      "Epoch 3 - Loss: 0.8854\n",
      "Epoch 4 - Loss: 0.8841\n",
      "Epoch 5 - Loss: 0.8830\n",
      "Epoch 6 - Loss: 0.8818\n",
      "Epoch 7 - Loss: 0.8806\n",
      "Epoch 8 - Loss: 0.8795\n",
      "Epoch 9 - Loss: 0.8785\n",
      "Epoch 10 - Loss: 0.8774\n",
      "Epoch 11 - Loss: 0.8764\n",
      "Epoch 12 - Loss: 0.8755\n",
      "Epoch 13 - Loss: 0.8747\n",
      "Epoch 14 - Loss: 0.8738\n",
      "Epoch 15 - Loss: 0.8730\n",
      "Epoch 16 - Loss: 0.8722\n",
      "Epoch 17 - Loss: 0.8715\n",
      "Epoch 18 - Loss: 0.8708\n",
      "Epoch 19 - Loss: 0.8701\n",
      "Epoch 20 - Loss: 0.8694\n",
      "Epoch 21 - Loss: 0.8688\n",
      "Epoch 22 - Loss: 0.8682\n",
      "Epoch 23 - Loss: 0.8676\n",
      "Epoch 24 - Loss: 0.8670\n",
      "Epoch 25 - Loss: 0.8664\n",
      "\n",
      " Example Predictions:\n",
      "Comment 0 ‚Üí Prediction: NO | Prob: 0.5170\n",
      "Comment 1 ‚Üí Prediction: NO | Prob: 0.5402\n",
      "Comment 2 ‚Üí Prediction: NO | Prob: 0.5840\n",
      "Comment 3 ‚Üí Prediction: NO | Prob: 0.5878\n",
      "Comment 4 ‚Üí Prediction: NO | Prob: 0.5818\n"
     ]
    }
   ],
   "source": [
    "# === 2. Convert to tensors\n",
    "X_tensor = torch.stack(X)                         # [4575, 256]\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)   # [4575]\n",
    "\n",
    "# === 3. Binary classifier\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=256):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).squeeze(1)\n",
    "\n",
    "classifier = BinaryClassifier()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "# === Compute class imbalance ratio\n",
    "yes_count = sum(y)\n",
    "no_count = len(y) - yes_count\n",
    "pos_weight = torch.tensor([no_count / yes_count])  # more weight to class 1 (yes)\n",
    "\n",
    "# === Weighted loss\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "# === 4. Training\n",
    "for epoch in range(25):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = classifier(X_tensor)         # [4575]\n",
    "    loss = criterion(logits, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === 5. Inference\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    logits = classifier(X_tensor)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    predictions = (probs >= 0.6).long()\n",
    "\n",
    "# === 6. Show predictions\n",
    "print(\"\\n Example Predictions:\")\n",
    "for i in range(min(5, len(predictions))):\n",
    "    label = \"YES\" if predictions[i].item() == 1 else \"NO\"\n",
    "    print(f\"Comment {i} ‚Üí Prediction: {label} | Prob: {probs[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ba149a-b62d-4ba9-b88f-7e01ae8f8c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "NO (not hate)       0.64      0.99      0.78      2914\n",
      "   YES (hate)       0.50      0.01      0.03      1661\n",
      "\n",
      "     accuracy                           0.64      4575\n",
      "    macro avg       0.57      0.50      0.40      4575\n",
      " weighted avg       0.59      0.64      0.50      4575\n",
      "\n",
      "\n",
      " Accuracy: 63.69%\n",
      "\n",
      " Confusion Matrix:\n",
      "[[2891   23]\n",
      " [1638   23]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Convert predictions to list\n",
    "preds = predictions.tolist()\n",
    "labels = y_tensor.tolist()\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"NO (not hate)\", \"YES (hate)\"]))\n",
    "acc = accuracy_score(labels, preds)\n",
    "print(f\"\\n Accuracy: {acc * 100:.2f}%\")\n",
    "print(\"\\n Confusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a1d6d-9f4f-4d51-b6e3-a2c44b2cd562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06019d45-7d9a-41f6-a24e-584f8228c12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd71f7-b9b7-4aaa-b939-76365add984a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f44b9-9808-440e-872f-98c0b655cc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485cdbe-7549-4e81-9830-d16eb46955c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7fed9-c21d-453d-9b3f-5e337c286e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cd5c6-6291-4114-ad62-089a27cf9c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbae83-3180-45e2-b3e2-84b157983367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f916e-1601-4de6-8b50-36ebbfec3c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b9d45-19a5-4e69-aada-bd274c4ace73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
