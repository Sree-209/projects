{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712fb4ec-1f43-489a-b4c0-90c0066364b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import chardet\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b33c44b-54e5-49ab-9f16-b21b338f861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/hate_speech.tsv\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read(100000))  # Read first 100KB\n",
    "    print(result[\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd4ccb0-23d0-4bcc-924c-bc5029b37fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codemixed</th>\n",
       "      <th>HateOrNot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>ye attankwadi Indian agent hai jo terrorism ph...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>bola na terrorism ko support karna band karoge...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>lagta hai aap ne movie dekhi hai which is writ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>tum log terrorism ko support karna band kardo ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>mujhe pehele se hi pata tha so Sallu fans ke b...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Codemixed HateOrNot\n",
       "4574  ye attankwadi Indian agent hai jo terrorism ph...        no\n",
       "4575  bola na terrorism ko support karna band karoge...        no\n",
       "4576  lagta hai aap ne movie dekhi hai which is writ...        no\n",
       "4577  tum log terrorism ko support karna band kardo ...        no\n",
       "4578  mujhe pehele se hi pata tha so Sallu fans ke b...       yes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\"Codemixed\", \"HateOrNot\"]  # Replace with actual column names\n",
    "hs_df = pd.read_csv(\"dataset/hate_speech.tsv\", \n",
    "                    encoding=result[\"encoding\"], \n",
    "                    sep=\"\\t\", header=None, \n",
    "                    names=column_names)\n",
    "\n",
    "hs_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0e10fb-397e-4c6a-9fad-cf08ebdb76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HateOrNot\n",
      "no     2914\n",
      "yes    1661\n",
      "n         2\n",
      "on        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(hs_df[\"HateOrNot\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ff5981-bb6e-4126-bbc5-fe1bb7cd528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_codemixed_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)         # remove links\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)                   # remove @ and the word after it\n",
    "    text = re.sub(r\"#\", \"\", text)                      # remove just the '#' symbol\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()           # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "hs_df[\"Codemixed\"] = hs_df[\"Codemixed\"].astype(str).apply(clean_codemixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31007c55-a4be-4bef-b35d-171aaa4a62ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4573    pehle confirm karo ke Mohammad ne sach mein 8 ...\n",
       "4574    ye attankwadi Indian agent hai jo terrorism ph...\n",
       "4575    bola na terrorism ko support karna band karoge...\n",
       "4576    lagta hai aap ne movie dekhi hai which is writ...\n",
       "4577    tum log terrorism ko support karna band kardo ...\n",
       "Name: Codemixed, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df[\"Codemixed\"][-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e80d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_df['Codemixed'].to_csv('code_mixed.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a5a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=code_mixed.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: code_mixed.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: code_mixed.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4579 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=468250\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=74\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4579 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=221590\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 24265 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4579\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 20113\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 20113 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12103 obj=11.1709 num_tokens=42534 num_tokens/piece=3.51434\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10641 obj=10.0563 num_tokens=42876 num_tokens/piece=4.02932\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7976 obj=10.0613 num_tokens=44951 num_tokens/piece=5.63578\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7965 obj=10.0061 num_tokens=44947 num_tokens/piece=5.64306\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5973 obj=10.1513 num_tokens=48118 num_tokens/piece=8.05592\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5973 obj=10.0932 num_tokens=48147 num_tokens/piece=8.06077\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4479 obj=10.2955 num_tokens=51668 num_tokens/piece=11.5356\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4479 obj=10.2301 num_tokens=51672 num_tokens/piece=11.5365\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3359 obj=10.46 num_tokens=55198 num_tokens/piece=16.4329\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3359 obj=10.3917 num_tokens=55196 num_tokens/piece=16.4323\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2519 obj=10.6469 num_tokens=58737 num_tokens/piece=23.3176\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2519 obj=10.5751 num_tokens=58739 num_tokens/piece=23.3184\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=10.7002 num_tokens=60374 num_tokens/piece=27.4427\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=10.6674 num_tokens=60375 num_tokens/piece=27.4432\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=code_mixed.txt --model_prefix=m --vocab_size=2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e06d5a0-98be-424a-bc7b-304dd33c587b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34019769-1a45-4653-94a6-07337144bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3205\n",
      "Test samples: 1374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "hs_df[\"HateOrNot\"] = hs_df[\"HateOrNot\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "train_df, test_df = train_test_split(hs_df, test_size=0.3, random_state=42, stratify=hs_df[\"HateOrNot\"])\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d21b2d-b58e-40c6-994e-36f839e3fc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments processed: 4579\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "vocab_size = 8000          # SentencePiece vocab size\n",
    "embedding_dim = 768        \n",
    "max_seq_len = 512          # Max number of tokens per comment\n",
    "\n",
    "# 1. Create embedding layers \n",
    "token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "pos_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "# Define one transformer encoder layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embedding_dim,         # 768 ‚Äî dimension of embeddings\n",
    "    nhead=8,                        # 8 attention heads\n",
    "    dim_feedforward=2048,          # size of feedforward layer\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    batch_first=True               # set to True to use [batch_size, seq_len, dim]\n",
    ")\n",
    "\n",
    "# Stack 12 such layers\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12)\n",
    "\n",
    "# 2. Container for final embeddings\n",
    "contextual_embeddings = {}\n",
    "\n",
    "for i, comment in enumerate(hs_df['Codemixed']):\n",
    "    ids = sp.encode_as_ids(comment)\n",
    "    if not ids:\n",
    "        continue\n",
    "\n",
    "    token_ids = torch.tensor(ids)\n",
    "    positions = torch.arange(len(token_ids))\n",
    "\n",
    "    # Get initial embeddings\n",
    "    token_embed_output = token_embedding(token_ids)\n",
    "    position_embed_output = pos_embedding(positions)\n",
    "    final_embedding = token_embed_output + position_embed_output  # [seq_len, 768]\n",
    "\n",
    "    # Add batch dimension: [1, seq_len, 768]\n",
    "    input_tensor = final_embedding.unsqueeze(0)\n",
    "\n",
    "    # Pass through 12-layer transformer\n",
    "    with torch.no_grad():  # unless you're training\n",
    "        transformer_output = transformer_encoder(input_tensor)  # [1, seq_len, 768]\n",
    "\n",
    "    # Option 1: Use mean pooling to get a single vector for the whole comment\n",
    "    contextual_embeddings[i] = transformer_output.squeeze(0)\n",
    "\n",
    "print(\"Total comments processed:\", len(contextual_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0008c36c-020d-4110-b81b-ee3ef658949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_size=128):\n",
    "        super(HANEncoder, self).__init__()\n",
    "        self.bigru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.attention_fc = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.context_vector = nn.Parameter(torch.rand(hidden_size * 2))\n",
    "\n",
    "    def forward(self, embeddings):  # [seq_len, input_dim]\n",
    "        x = embeddings.unsqueeze(0)  # [1, seq_len, input_dim]\n",
    "        gru_out, _ = self.bigru(x)   # [1, seq_len, 2*hidden]\n",
    "\n",
    "        u = torch.tanh(self.attention_fc(gru_out))         # [1, seq_len, 2*hidden]\n",
    "        alpha = torch.matmul(u, self.context_vector)       # [1, seq_len]\n",
    "        alpha = F.softmax(alpha, dim=-1)                   # [1, seq_len]\n",
    "\n",
    "        # Weighted sum\n",
    "        sentence_vector = torch.sum(gru_out * alpha.unsqueeze(-1), dim=1)  # [1, 2*hidden]\n",
    "\n",
    "        return sentence_vector.squeeze(0), alpha.squeeze(0)  # return both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886b0f0d-984f-4685-8c32-9882c4081add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4579/4579 [00:05<00:00, 816.99it/s] \n"
     ]
    }
   ],
   "source": [
    "han_model = HANEncoder(input_dim=768, hidden_size=128)\n",
    "han_model.eval()\n",
    "\n",
    "final_vectors = {}\n",
    "attention_scores = {}\n",
    "\n",
    "for idx, embedding in tqdm(contextual_embeddings.items()):\n",
    "    if embedding.shape[0] < 2:\n",
    "        continue  # skip very short ones\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sentence_vector, attn_weights = han_model(embedding)\n",
    "\n",
    "        final_vectors[idx] = sentence_vector  # [256]\n",
    "        attention_scores[idx] = attn_weights  # [seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983e6dfe-7f75-4761-9f92-7e8fb80e5aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipped idx 3689: label was 'n' (not yes/no)\n",
      "‚ö†Ô∏è Skipped idx 3763: label was 'on' (not yes/no)\n",
      "‚ö†Ô∏è Skipped idx 3874: label was 'n' (not yes/no)\n",
      "‚ö†Ô∏è Skipped idx 4407: label was 'on' (not yes/no)\n",
      "\n",
      "‚úÖ Final usable vectors: 4575\n",
      "‚ùå Skipped: 4\n"
     ]
    }
   ],
   "source": [
    "label_map = {\"yes\": 1, \"no\": 0}\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "skipped = 0\n",
    "\n",
    "for idx, vec in final_vectors.items():\n",
    "    raw_label = hs_df.loc[idx, 'HateOrNot']\n",
    "    label = label_map.get(str(raw_label).strip().lower())\n",
    "\n",
    "    if label is None:\n",
    "        print(f\"‚ö†Ô∏è Skipped idx {idx}: label was '{raw_label}' (not yes/no)\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    if not isinstance(vec, torch.Tensor):\n",
    "        print(f\"‚ö†Ô∏è Skipped idx {idx}: vector is not a tensor\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    if vec.shape != torch.Size([256]):\n",
    "        print(f\"‚ö†Ô∏è Skipped idx {idx}: vector has shape {vec.shape} (expected [256])\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    X.append(vec)\n",
    "    y.append(label)\n",
    "\n",
    "print(f\"\\n‚úÖ Final usable vectors: {len(X)}\")\n",
    "print(f\"‚ùå Skipped: {skipped}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a03582fe-50ce-447e-86dc-04ebff2995a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_vectors type: <class 'dict'>\n",
      "final_vectors length: 4579\n",
      "Key: 0, Shape: torch.Size([256]), Type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(\"final_vectors type:\", type(final_vectors))\n",
    "print(\"final_vectors length:\", len(final_vectors))\n",
    "\n",
    "# Print a sample entry if exists\n",
    "for k, v in final_vectors.items():\n",
    "    print(f\"Key: {k}, Shape: {getattr(v, 'shape', 'Not a tensor')}, Type: {type(v)}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9bfade5-4aa3-4830-9e40-6faf45c5e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.9143\n",
      "Epoch 2 - Loss: 0.9015\n",
      "Epoch 3 - Loss: 0.8936\n",
      "Epoch 4 - Loss: 0.8904\n",
      "Epoch 5 - Loss: 0.8904\n",
      "Epoch 6 - Loss: 0.8919\n",
      "Epoch 7 - Loss: 0.8931\n",
      "Epoch 8 - Loss: 0.8930\n",
      "Epoch 9 - Loss: 0.8917\n",
      "Epoch 10 - Loss: 0.8897\n",
      "Epoch 11 - Loss: 0.8875\n",
      "Epoch 12 - Loss: 0.8856\n",
      "Epoch 13 - Loss: 0.8844\n",
      "Epoch 14 - Loss: 0.8837\n",
      "Epoch 15 - Loss: 0.8835\n",
      "Epoch 16 - Loss: 0.8835\n",
      "Epoch 17 - Loss: 0.8833\n",
      "Epoch 18 - Loss: 0.8829\n",
      "Epoch 19 - Loss: 0.8822\n",
      "Epoch 20 - Loss: 0.8812\n",
      "Epoch 21 - Loss: 0.8802\n",
      "Epoch 22 - Loss: 0.8792\n",
      "Epoch 23 - Loss: 0.8784\n",
      "Epoch 24 - Loss: 0.8779\n",
      "Epoch 25 - Loss: 0.8775\n",
      "\n",
      "üì¢ Example Predictions:\n",
      "Comment 0 ‚Üí Prediction: NO | Prob: 0.5449\n",
      "Comment 1 ‚Üí Prediction: NO | Prob: 0.5306\n",
      "Comment 2 ‚Üí Prediction: NO | Prob: 0.5679\n",
      "Comment 3 ‚Üí Prediction: NO | Prob: 0.5593\n",
      "Comment 4 ‚Üí Prediction: NO | Prob: 0.5309\n"
     ]
    }
   ],
   "source": [
    "# === 2. Convert to tensors\n",
    "X_tensor = torch.stack(X)                         # [4575, 256]\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)   # [4575]\n",
    "\n",
    "# === 3. Binary classifier\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=256):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x).squeeze(1)\n",
    "\n",
    "classifier = BinaryClassifier()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "# === Compute class imbalance ratio\n",
    "yes_count = sum(y)\n",
    "no_count = len(y) - yes_count\n",
    "pos_weight = torch.tensor([no_count / yes_count])  # more weight to class 1 (yes)\n",
    "\n",
    "# === Weighted loss\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "# === 4. Training\n",
    "for epoch in range(25):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = classifier(X_tensor)         # [4575]\n",
    "    loss = criterion(logits, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# === 5. Inference\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    logits = classifier(X_tensor)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    predictions = (probs >= 0.6).long()\n",
    "\n",
    "# === 6. Show predictions\n",
    "print(\"\\nüì¢ Example Predictions:\")\n",
    "for i in range(min(5, len(predictions))):\n",
    "    label = \"YES\" if predictions[i].item() == 1 else \"NO\"\n",
    "    print(f\"Comment {i} ‚Üí Prediction: {label} | Prob: {probs[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ba149a-b62d-4ba9-b88f-7e01ae8f8c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "NO (not hate)       0.64      0.99      0.78      2914\n",
      "   YES (hate)       0.49      0.01      0.02      1661\n",
      "\n",
      "     accuracy                           0.64      4575\n",
      "    macro avg       0.56      0.50      0.40      4575\n",
      " weighted avg       0.58      0.64      0.50      4575\n",
      "\n",
      "\n",
      "‚úÖ Accuracy: 63.67%\n",
      "\n",
      "üìâ Confusion Matrix:\n",
      "[[2894   20]\n",
      " [1642   19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Convert predictions to list\n",
    "preds = predictions.tolist()\n",
    "labels = y_tensor.tolist()\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"NO (not hate)\", \"YES (hate)\"]))\n",
    "acc = accuracy_score(labels, preds)\n",
    "print(f\"\\n‚úÖ Accuracy: {acc * 100:.2f}%\")\n",
    "print(\"\\nüìâ Confusion Matrix:\")\n",
    "print(confusion_matrix(labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a1d6d-9f4f-4d51-b6e3-a2c44b2cd562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06019d45-7d9a-41f6-a24e-584f8228c12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd71f7-b9b7-4aaa-b939-76365add984a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f44b9-9808-440e-872f-98c0b655cc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485cdbe-7549-4e81-9830-d16eb46955c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7fed9-c21d-453d-9b3f-5e337c286e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cd5c6-6291-4114-ad62-089a27cf9c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbae83-3180-45e2-b3e2-84b157983367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f916e-1601-4de6-8b50-36ebbfec3c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b9d45-19a5-4e69-aada-bd274c4ace73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
