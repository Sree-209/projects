{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "712fb4ec-1f43-489a-b4c0-90c0066364b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import chardet\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b33c44b-54e5-49ab-9f16-b21b338f861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/hate_speech.tsv\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read(100000))  # Read first 100KB\n",
    "    print(result[\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdd4ccb0-23d0-4bcc-924c-bc5029b37fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codemixed</th>\n",
       "      <th>HateOrNot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>ye attankwadi Indian agent hai jo terrorism ph...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>bola na terrorism ko support karna band karoge...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>lagta hai aap ne movie dekhi hai which is writ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>tum log terrorism ko support karna band kardo ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>mujhe pehele se hi pata tha so Sallu fans ke b...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Codemixed HateOrNot\n",
       "4574  ye attankwadi Indian agent hai jo terrorism ph...        no\n",
       "4575  bola na terrorism ko support karna band karoge...        no\n",
       "4576  lagta hai aap ne movie dekhi hai which is writ...        no\n",
       "4577  tum log terrorism ko support karna band kardo ...        no\n",
       "4578  mujhe pehele se hi pata tha so Sallu fans ke b...       yes"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\"Codemixed\", \"HateOrNot\"]  # Replace with actual column names\n",
    "hs_df = pd.read_csv(\"dataset/hate_speech.tsv\", encoding=result[\"encoding\"], sep=\"\\t\", header=None, names=column_names)\n",
    "\n",
    "hs_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cd2be53-16fa-4493-93e4-ddbee4b67898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-1252\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset/Hinglish_Profanity_List.csv\", \"rb\") as file:\n",
    "    raw_data = file.read()\n",
    "    result = chardet.detect(raw_data)\n",
    "    print(result[\"encoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e718b7c4-8ac9-43dd-afd6-e0a07d2c09e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>badir</th>\n",
       "      <th>idiot</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>vahiyaat</td>\n",
       "      <td>disgusting</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>jihadi</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>atankvadi</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>atankwadi</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>aatanki</td>\n",
       "      <td>terorist</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         badir       idiot  1\n",
       "203   vahiyaat  disgusting  4\n",
       "204     jihadi   terrorist  4\n",
       "205  atankvadi   terrorist  4\n",
       "206  atankwadi   terrorist  4\n",
       "207    aatanki    terorist  4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_df = pd.read_csv(\"dataset/Hinglish_Profanity_List.csv\", encoding=result['encoding'])\n",
    "\n",
    "pf_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0ff5981-bb6e-4126-bbc5-fe1bb7cd528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_codemixed_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)         # remove links\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)                   # remove @ and the word after it\n",
    "    text = re.sub(r\"#\", \"\", text)                      # remove just the '#' symbol\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()           # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply to 'Codemixed' column\n",
    "hs_df[\"Codemixed\"] = hs_df[\"Codemixed\"].astype(str).apply(clean_codemixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31007c55-a4be-4bef-b35d-171aaa4a62ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4573    pehle confirm karo ke Mohammad ne sach mein 8 ...\n",
       "4574    ye attankwadi Indian agent hai jo terrorism ph...\n",
       "4575    bola na terrorism ko support karna band karoge...\n",
       "4576    lagta hai aap ne movie dekhi hai which is writ...\n",
       "4577    tum log terrorism ko support karna band kardo ...\n",
       "Name: Codemixed, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_df[\"Codemixed\"][-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64e80d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_df['Codemixed'].to_csv('code_mixed.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45a5a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=code_mixed.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: code_mixed.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: code_mixed.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4579 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=468250\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9513% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=74\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999513\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4579 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=221590\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 24265 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4579\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 20113\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 20113 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12103 obj=11.1709 num_tokens=42534 num_tokens/piece=3.51434\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10641 obj=10.0563 num_tokens=42876 num_tokens/piece=4.02932\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7976 obj=10.0613 num_tokens=44951 num_tokens/piece=5.63578\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7965 obj=10.0061 num_tokens=44947 num_tokens/piece=5.64306\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5973 obj=10.1513 num_tokens=48118 num_tokens/piece=8.05592\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5973 obj=10.0932 num_tokens=48147 num_tokens/piece=8.06077\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4479 obj=10.2955 num_tokens=51668 num_tokens/piece=11.5356\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4479 obj=10.2301 num_tokens=51672 num_tokens/piece=11.5365\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3359 obj=10.46 num_tokens=55198 num_tokens/piece=16.4329\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3359 obj=10.3917 num_tokens=55196 num_tokens/piece=16.4323\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2519 obj=10.6469 num_tokens=58737 num_tokens/piece=23.3176\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2519 obj=10.5751 num_tokens=58739 num_tokens/piece=23.3184\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=10.7002 num_tokens=60374 num_tokens/piece=27.4427\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=10.6674 num_tokens=60375 num_tokens/piece=27.4432\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=code_mixed.txt --model_prefix=m --vocab_size=2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e06d5a0-98be-424a-bc7b-304dd33c587b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59d21b2d-b58e-40c6-994e-36f839e3fc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pieces: ['▁Tu', '▁kitna', '▁gand', 'u', '▁hai']\n",
      "IDs: [597, 690, 555, 39, 3]\n"
     ]
    }
   ],
   "source": [
    "pieces = sp.encode_as_pieces(\"Tu kitna gandu hai\")\n",
    "ids = sp.encode_as_ids(\"Tu kitna gandu hai\")\n",
    "\n",
    "print(\"Pieces:\", pieces)\n",
    "print(\"IDs:\", ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0008c36c-020d-4110-b81b-ee3ef658949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor(ids)  # shape: [seq_len]\n",
    "\n",
    "# Settings\n",
    "vocab_size = 8000         # Set this to your SentencePiece vocab size\n",
    "embedding_dim = 768       # You can use 300, 512, 768 etc.\n",
    "max_seq_len = 512         # Maximum length of your sequences\n",
    "\n",
    "# 1. Token Embedding layer\n",
    "token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "token_embed_output = token_embedding(token_ids)  # shape: [seq_len, embedding_dim]\n",
    "\n",
    "# 2. Positional Embedding layer\n",
    "pos_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "\n",
    "# Create position IDs: [0, 1, 2, ..., seq_len-1]\n",
    "positions = torch.arange(0, len(token_ids)).unsqueeze(0)  # shape: [1, seq_len]\n",
    "position_embed_output = pos_embedding(positions).squeeze(0)  # shape: [seq_len, embedding_dim]\n",
    "\n",
    "# 3. Add token embeddings and positional embeddings\n",
    "final_embedding = token_embed_output + position_embed_output  # shape: [seq_len, embedding_dim]\n",
    "\n",
    "# Now `final_embedding` is ready to feed into RNN/HAN etc.\n",
    "print(final_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b0f0d-984f-4685-8c32-9882c4081add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26b3dc-35ac-4162-92ef-b7607accbc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e6dfe-7f75-4761-9f92-7e8fb80e5aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03582fe-50ce-447e-86dc-04ebff2995a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfade5-4aa3-4830-9e40-6faf45c5e4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba149a-b62d-4ba9-b88f-7e01ae8f8c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a1d6d-9f4f-4d51-b6e3-a2c44b2cd562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
